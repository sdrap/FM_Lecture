# Expected Shortfall

So far we investigate the fundamental of risk assessment in terms of what it shall satisfy in order to reflect sound quantification, namely **monotonicity** and **diversification** and for financial purposes **cash-invariance**.
This allowed us to point to the fundamental flaws of mean variance or value at risk towards this goal.
However, from a practical viewpoint we are not very much advanced in terms of what we should consider.

Considering a risk quantification instrument $R$ we shall keep in mind the following points:

1. **Sound:** The instrument $R$ shall be sound in terms of risk quantification, that is satisfies **diversification** and **monotonicity** properties.
2. **Understandable:** The instrument $R$ shall be intuitively understood from a financial viewpoint for person not versed into the arcane of mathematics.
    In the end you have to convince your boss, the regulator, the public, that what you are using does make sense.
3. **Implementable:** You should be able to implement the computation. 
    By then end of the day, you need to provide a number, hence, it should be possible out of data to program a function that will deliver the value of your risk assessment (prototyping).
4. **Efficient/Robust:** The implementation should be industry ready, that is fast, bug-free and reliable.
    Indeed, it is not a notebook that will run this computations, but every day, you reliable have to be able to aggregate and compute possibly very large and complex position and provide a result in a timely manner (by regulation, the monetary requirement of large financial institutions have to be provided on a daily basis).


So far we only laid down the ground work for the first point, the other points are each in their own regard very important.
Since the financial crisis in 2008, the shortcomings of value at risk had been laid bare, and even if those shortcomings (the first point) were known by academics, it took some time to get the other points done, so that an new industry standard could emerge, namely the **expected shortfall** (also know under different though equivalent flavors as **average value at risk** or **conditional value at risk**).

## Expected Shortfall

As the main issue of value at risk being the fact that it only provides information at one point of the CDF and being blind beyond it, the idea is to consider the tail beyond value at risk


!!! definition "Definition: Expected Shortfall"

    The expected shortfall of a random variable (integrable) at level $\alpha$ is defined as

    \[
        ES_{\alpha}(L) = \frac{1}{\alpha}\int_0^\alpha V@R_{s}(L) ds = \frac{1}{\alpha}\int_{1-\alpha}^1 q_L(s) ds
    \]


As showed in the picture, the expected shortfall tries to cover the shortcomings of value at risk by covering the loss area beyond V@R.
Indeed, if $\tilde{L}$ and $L$ have the same value at risk however with $\tilde{L}$ occurring larger losses beyond value at risk (that is with fatter tails than $L$), then even if they have the same value at risk, the expected shortfall (the area beyond V@R) of $\tilde{L}$ is going to be larger than $L$.

This argument does answer the second point of our wish list as it seems natural after recognising the shortcoming of V@R in terms of distribution, it still does not answer the first point in the first place.
It seems kind of strange that while V@R does not satisfies diversification ES should do.
Indeed, it is pretty straight forward to see that the good properties of V@R (monotonicity, law invariance, and cash-invariance as well as positive homogeneity) do carry over through the integral to the expected shortfall.
However, since V@R is not convex, it is truly not clear from this representation why it should be convex.

Furthermore, even if this representation satisfies the third point (it is just an integral of an object we can compute), there are strong doubts, that it is an efficient method.
Indeed, computing this integral of the quantile requires to compute many quantiles between $1-\alpha$ and $1$ which is quite intensive and also prone to error as the quantile in extreme values of the distribution (for instance at level $99.999\%$ of $99.99999\%$) starts to be unstable (requires to sample the distribution in areas where it is very unlikely to happen).

In order to answer those questions, let us study another class of risk assessment instruments introduced by the operation research scientists Ben-Tal and Teboulle, namely, the optimized certainty equivalent.

## Optimized Certainty Equivalent

!!! definition "Definition: Loss Function"
    A function $\ell \colon \mathbb{R} \to \mathbb{R}$ is called a loss function if

    * $\ell$ is convex
    * $\ell$ is increasing
    
    * $\ell(0) = 0$ and $\ell^\prime(0) = 1$(1)
        {.annotate}

        1.  Note that $\ell$ does not necessarily need to be differentiable such as $\ell(x) = x^+/\alpha$ for $0< \alpha <1$. It just needs to have $\ell^{\prime}_-(0) \leq 1 \leq \ell^\prime_+(0)$ where $\ell_-^\prime$ and $\ell^\prime_+$ are the left and right derivative that always exists for convex functions.

    * $\lim_{x \to \infty}\ell(x)/x >1$ and $\lim_{x \to -\infty} \ell(x)/x <1$.


!!! definition "Definition: Optimized Certainty Equivalent"

    Given a loss function $\ell$, the optimized certainty equivalent $R$ is defined as

    \[
      R(L) = \inf \left\{ m + E\left[ \ell(L - m) \right] \colon m \in \mathbb{R}\right\}
    \]

!!! proposition

    Given a loss function $\ell$, the optimized certainty equivalent $R$ is a cash-invariant, law-invariant risk measure.

    Furthermore it holds that

    \[
      R(L) = m^\ast +E\left[ \ell(L- m^\ast) \right]
    \]
    
    where(1)
    {.annotate}

    1.  If $\ell$ is not differentiable at $0$, then it changes to 

        \[E[\ell^\prime_-(L-m^\ast)] \leq 1 \leq E\left[ \ell^\prime_+(L-m^\ast) \right]\]

    \[
      E[\ell^\prime(L - m^\ast)] = 1
    \]


!!! proof


